{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "rUEcCIJSPeMY"
      },
      "outputs": [],
      "source": [
        "# General Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "-Hvs2ZJKPAPY"
      },
      "outputs": [],
      "source": [
        "# Load in the training data\n",
        "a_train = pd.read_csv('aortaP_train_data.csv')\n",
        "b_train = pd.read_csv('brachP_train_data.csv')\n",
        "\n",
        "# Get rid of extra column\n",
        "a_train = a_train.drop('Unnamed: 0', axis=1)\n",
        "b_train = b_train.drop('Unnamed: 0', axis=1)\n",
        "\n",
        "a_test = pd.read_csv('aortaP_test_data.csv')\n",
        "b_test = pd.read_csv('brachP_test_data.csv')\n",
        "\n",
        "a_test = a_test.drop('Unnamed: 0', axis=1)\n",
        "b_test = b_test.drop('Unnamed: 0', axis=1)\n",
        "\n",
        "a_test['target'] = 0\n",
        "b_test['target'] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "iPpstuGuBXwI"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import pandas as pd\n",
        "\n",
        "# Function to smooth data using a moving average\n",
        "def smooth_data(df, window_size=5):\n",
        "    \"\"\"\n",
        "    Apply a moving average to smooth each row in a DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    - df: The DataFrame to smooth\n",
        "    - window_size: Size of the moving window\n",
        "\n",
        "    Returns:\n",
        "    - A smoothed DataFrame\n",
        "    \"\"\"\n",
        "    smoothed_df = df.apply(lambda row: row.rolling(window=window_size, min_periods=1).mean(), axis=1)\n",
        "    return smoothed_df\n",
        "\n",
        "# Fill in the NaNs\n",
        "# Interpolate all of the rows for a and b\n",
        "a_train = a_train.interpolate(method='linear', axis=1)\n",
        "b_train = b_train.interpolate(method='linear', axis=1)\n",
        "\n",
        "a_test = a_test.interpolate(method='linear', axis=1)\n",
        "b_test = b_test.interpolate(method='linear', axis=1)\n",
        "\n",
        "# Backward fill any remaining NaNs\n",
        "a_train = a_train.bfill(axis=1)\n",
        "b_train = b_train.bfill(axis=1)\n",
        "\n",
        "a_test = a_test.bfill(axis=1)\n",
        "b_test = b_test.bfill(axis=1)\n",
        "\n",
        "# Apply smoothing\n",
        "a_train_smoothed = smooth_data(a_train.drop(columns=['target']), window_size=3)\n",
        "b_train_smoothed = smooth_data(b_train.drop(columns=['target']), window_size=3)\n",
        "\n",
        "a_test_smoothed = smooth_data(a_test.drop(columns=['target']), window_size=3)\n",
        "b_test_smoothed = smooth_data(b_test.drop(columns=['target']), window_size=3)\n",
        "\n",
        "# Add the target column back after smoothing\n",
        "a_train_smoothed['target'] = a_train['target']\n",
        "b_train_smoothed['target'] = b_train['target']\n",
        "\n",
        "a_test_smoothed['target'] = a_test['target']\n",
        "b_test_smoothed['target'] = b_test['target']\n",
        "\n",
        "# Get a scaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Normalize only the columns except target in a_train\n",
        "a_train_scaled = a_train_smoothed.drop(columns=['target'])\n",
        "a_train_scaled = pd.DataFrame(scaler.fit_transform(a_train_scaled), columns=a_train_scaled.columns)\n",
        "\n",
        "a_test_scaled = a_test_smoothed.drop(columns=['target'])\n",
        "a_test_scaled = pd.DataFrame(scaler.fit_transform(a_test_scaled), columns=a_test_scaled.columns)\n",
        "\n",
        "# Normalize only the columns except target in b_train\n",
        "b_train_scaled = b_train_smoothed.drop(columns=['target'])\n",
        "b_train_scaled = pd.DataFrame(scaler.fit_transform(b_train_scaled), columns=b_train_scaled.columns)\n",
        "\n",
        "b_test_scaled = b_test_smoothed.drop(columns=['target'])\n",
        "b_test_scaled = pd.DataFrame(scaler.fit_transform(b_test_scaled), columns=b_test_scaled.columns)\n",
        "\n",
        "# Add the target column back to the DataFrame\n",
        "b_train_scaled['target'] = b_train['target']\n",
        "\n",
        "b_test_scaled['target'] = b_test['target']\n",
        "\n",
        "# Make a combined dataframe\n",
        "c_train = pd.concat([a_train_scaled, b_train_scaled], axis=1)\n",
        "c_test = pd.concat([a_test_scaled, b_test_scaled], axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "d2Wzqf9TP8fy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "47779878-a7a0-4a17-f272-219290b0d157"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1: Train size = 2015, Test size = 504\n",
            "Fold 2: Train size = 2015, Test size = 504\n",
            "Fold 3: Train size = 2015, Test size = 504\n",
            "Fold 4: Train size = 2015, Test size = 504\n",
            "Fold 5: Train size = 2016, Test size = 503\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "# Now let's split our data\n",
        "# We will first partion our train into model dev and holdout (90/10)\n",
        "dev_set, holdout_set = train_test_split(c_train, test_size=0.1, random_state=42, stratify=c_train['target'])\n",
        "\n",
        "# Reset index\n",
        "holdout_set = holdout_set.reset_index(drop=True)\n",
        "dev_set = dev_set.reset_index(drop=True)\n",
        "\n",
        "\n",
        "# Train/Val/Test split (80/10/10)\n",
        "# Get train and temp\n",
        "train_set, temp_set = train_test_split(dev_set, test_size=0.2, random_state=42, stratify=dev_set['target'])\n",
        "# Get val and test\n",
        "val_set, test_set = train_test_split(temp_set, test_size=0.5, random_state=42, stratify=temp_set['target'])\n",
        "\n",
        "# Reset indicies\n",
        "train_set = train_set.reset_index(drop=True)\n",
        "val_set = val_set.reset_index(drop=True)\n",
        "test_set = test_set.reset_index(drop=True)\n",
        "\n",
        "# Now, add 5-fold cross-validation for train_set (90% train / 10% test for each fold)\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Preparing the cross-validation splits\n",
        "folds = []\n",
        "for fold_idx, (train_index, test_index) in enumerate(skf.split(train_set, train_set['target'])):\n",
        "    train_fold = train_set.iloc[train_index].reset_index(drop=True)\n",
        "    test_fold = train_set.iloc[test_index].reset_index(drop=True)\n",
        "    folds.append((train_fold, test_fold))\n",
        "    print(f\"Fold {fold_idx + 1}: Train size = {len(train_fold)}, Test size = {len(test_fold)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "z2gtjWVmqvKA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Create dataset\n",
        "class ABDataset(Dataset):\n",
        "    def __init__(self, c_train):\n",
        "        \"\"\"\n",
        "        Custom dataset to return rows from a_train and b_train as a tuple,\n",
        "        and generate a multilabel target\n",
        "        Args:\n",
        "            c_train (pd.DataFrame): Time series data for A and B\n",
        "        \"\"\"\n",
        "        self.c_train = c_train\n",
        "        self.a_train = c_train.iloc[:, :c_train.shape[1]//2]\n",
        "        self.b_train = c_train.iloc[:, c_train.shape[1]//2:]\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Return the number of samples (rows) in the dataset.\n",
        "        \"\"\"\n",
        "        return len(self.a_train)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Get a single sample (pair of rows and a multilabel target) from the dataset.\n",
        "        Args:\n",
        "            idx (int): Index for retrieving a sample.\n",
        "        \"\"\"\n",
        "        # Get the rows from both datasets\n",
        "        a_row = self.a_train.iloc[idx].values\n",
        "        b_row = self.b_train.iloc[idx].drop('target').values\n",
        "\n",
        "        # get the target value (Arbitarily from A)\n",
        "        target = self.b_train.iloc[idx]['target'].astype(int)\n",
        "\n",
        "        # Create the multilabel target\n",
        "        # Set first t\n",
        "        label = np.zeros(6)\n",
        "        label[:(target + 1)] = 1\n",
        "\n",
        "\n",
        "        # Create a single tensor with 2 channels: one for a_row, one for b_row\n",
        "        combined_tensor = np.stack([a_row, b_row], axis=0)  # Shape will be (2, 336)\n",
        "\n",
        "        # Convert to torch tensor\n",
        "        combined_tensor = torch.tensor(combined_tensor, dtype=torch.float32)\n",
        "        target_tensor = torch.tensor(label, dtype=torch.float32)\n",
        "\n",
        "        return combined_tensor, target_tensor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "CpPUNpVAC8eJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class ABCNN(nn.Module):\n",
        "    def __init__(self, input_channels=2, num_classes=6):\n",
        "        super(ABCNN, self).__init__()\n",
        "\n",
        "        # First convolutional layer\n",
        "        self.conv1 = nn.Conv1d(input_channels, 64, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.dropout1 = nn.Dropout(0.2)\n",
        "        self.pool1 = nn.AvgPool1d(kernel_size=2, stride=3)\n",
        "\n",
        "        # Second convolutional layer\n",
        "        self.conv2 = nn.Conv1d(64, 128, kernel_size=5, padding=2)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.dropout2 = nn.Dropout(0.3)\n",
        "        self.pool2 = nn.AvgPool1d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Third convolutional layer\n",
        "        self.conv3 = nn.Conv1d(128, 256, kernel_size=7, padding=3)\n",
        "        self.bn3 = nn.BatchNorm1d(256)\n",
        "        self.dropout3 = nn.Dropout(0.4)\n",
        "        self.pool3 = nn.AvgPool1d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.fc = nn.Linear(256 * 28, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Convolutional layers\n",
        "        x = torch.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.dropout1(self.pool1(x))\n",
        "\n",
        "        x = torch.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.dropout2(self.pool2(x))\n",
        "\n",
        "        x = torch.relu(self.bn3(self.conv3(x)))\n",
        "        x = self.dropout3(self.pool3(x))\n",
        "\n",
        "        # Flatten the output for the fully connected layer\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # Fully connected layer with activation function\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "oFHppJwCJgxS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "0ea06ade-89bc-45b4-ede0-c9f81309bfd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataLoaders for Fold 1 created: Train Loader size = 32, Test Loader size = 8\n",
            "DataLoaders for Fold 2 created: Train Loader size = 32, Test Loader size = 8\n",
            "DataLoaders for Fold 3 created: Train Loader size = 32, Test Loader size = 8\n",
            "DataLoaders for Fold 4 created: Train Loader size = 32, Test Loader size = 8\n",
            "DataLoaders for Fold 5 created: Train Loader size = 32, Test Loader size = 8\n"
          ]
        }
      ],
      "source": [
        "# Create dataset\n",
        "train_dataset = ABDataset(train_set)\n",
        "val_dataset = ABDataset(val_set)\n",
        "test_dataset = ABDataset(test_set)\n",
        "\n",
        "dev_dataset = ABDataset(dev_set)\n",
        "holdout_dataset = ABDataset(holdout_set)\n",
        "big_test = ABDataset(c_test)\n",
        "\n",
        "# Create DataLoader instances for batching\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=64, shuffle=True)\n",
        "holdout_loader = DataLoader(holdout_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "big_test_loader = DataLoader(big_test, batch_size=64, shuffle=False)\n",
        "\n",
        "\n",
        "# DataLoaders for 5-Fold Cross-Validation\n",
        "fold_loaders = []  # To store loaders for each fold\n",
        "\n",
        "for fold_idx, (train_fold, test_fold) in enumerate(folds):\n",
        "    # Create datasets for the current fold\n",
        "    train_fold_dataset = ABDataset(train_fold)\n",
        "    test_fold_dataset = ABDataset(test_fold)\n",
        "\n",
        "    # Create DataLoader instances for batching\n",
        "    train_fold_loader = DataLoader(train_fold_dataset, batch_size=64, shuffle=True)\n",
        "    test_fold_loader = DataLoader(test_fold_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "    # Store the loaders as a tuple (train_loader, test_loader) for this fold\n",
        "    fold_loaders.append((train_fold_loader, test_fold_loader))\n",
        "    print(f\"DataLoaders for Fold {fold_idx + 1} created: Train Loader size = {len(train_fold_loader)}, Test Loader size = {len(test_fold_loader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CROSS VALIDATION\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torchsummary import summary\n",
        "\n",
        "# Variables to track predictions for each fold\n",
        "all_predictions = []\n",
        "all_true_labels = []\n",
        "\n",
        "best_model = None\n",
        "# Loop through the 5 folds\n",
        "for fold_idx, (train_fold_loader, val_fold_loader) in enumerate(fold_loaders):\n",
        "    print(f\"\\nStarting Fold {fold_idx + 1}\")\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    # Reinitialize model weights for each fold\n",
        "    model = ABCNN()  # Reinitialize the model\n",
        "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"Total number of trainable parameters: {total_params}\")\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-3)  # Reinitialize optimizer\n",
        "    scheduler = StepLR(optimizer, step_size=7, gamma=0.5)  # Reinitialize scheduler\n",
        "    criterion = torch.nn.BCEWithLogitsLoss()\n",
        "    num_epochs = 45\n",
        "    # Train and validate for the specified number of epochs\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for c_batch, target_batch in train_fold_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(c_batch)\n",
        "            loss = criterion(outputs, target_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = running_loss / len(train_fold_loader)\n",
        "        print(f\"Fold {fold_idx + 1}, Epoch {epoch + 1}/{num_epochs}, Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct_normal = 0\n",
        "        total_samples = 0\n",
        "        off_by_1_count = 0\n",
        "        off_by_2_count = 0\n",
        "        epoch_preds = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for c_batch, target_batch in val_fold_loader:\n",
        "                outputs = model(c_batch)\n",
        "                loss = criterion(outputs, target_batch)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                predicted = torch.sigmoid(outputs) > 0.5\n",
        "                for i in range(len(predicted)):\n",
        "                    predicted_int = predicted[i].cpu().int().numpy().sum()\n",
        "                    target_int = target_batch[i].cpu().int().numpy().sum()\n",
        "\n",
        "                    # Save predictions and true labels\n",
        "                    if epoch == num_epochs - 1:  # Save for the last epoch\n",
        "                        all_predictions.append(predicted_int)\n",
        "                        all_true_labels.append(target_int)\n",
        "\n",
        "                # Track accuracy\n",
        "                for i in range(len(target_batch)):\n",
        "                    target_values = target_batch[i]\n",
        "                    predicted_values = predicted[i]\n",
        "                    if torch.all(target_values == predicted_values):\n",
        "                        correct_normal += 1\n",
        "\n",
        "                    abs_diff = torch.abs(predicted_values.int() - target_values.int()).sum()\n",
        "                    off_by_1_count += (abs_diff == 1).item()\n",
        "                    off_by_2_count += (abs_diff == 2).item()\n",
        "                    total_samples += 1\n",
        "\n",
        "        # Calculate validation metrics\n",
        "        avg_val_loss = val_loss / len(val_fold_loader)\n",
        "        normal_accuracy = correct_normal / total_samples\n",
        "        print(f\"Fold {fold_idx + 1}, Epoch {epoch + 1}/{num_epochs}, Validation Loss: {avg_val_loss:.4f}\")\n",
        "        print(f\"Normal Accuracy: {normal_accuracy:.4f}\")\n",
        "        print(f\"Off-by-1 Accuracy: {(off_by_1_count / total_samples):.4f}\")\n",
        "        print(f\"Off-by-2 Accuracy: {(off_by_2_count / total_samples):.4f}\")\n",
        "\n",
        "        # Save the best model for this fold\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            torch.save(model.state_dict(), f\"best_model_fold_{fold_idx + 1}.pth\")\n",
        "            print(f\"Saved best model for Fold {fold_idx + 1} with validation loss {best_val_loss:.4f}\")\n",
        "            best_model = model\n",
        "        # Step the learning rate scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "# Save predictions and true labels for all folds\n",
        "predictions_df = pd.DataFrame({\n",
        "    \"True Labels\": all_true_labels,\n",
        "    \"Predictions\": all_predictions\n",
        "})\n",
        "predictions_df.to_csv(\"crossval_predictions.csv\", index=False)\n",
        "print(\"Predictions saved to crossval_predictions.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "collapsed": true,
        "id": "mi7BrXYo_H8g",
        "outputId": "2311c0b9-fc07-4e22-8bce-74f0a7d4b9a5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting Fold 1\n",
            "Total number of trainable parameters: 315078\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-24aa28419e49>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Final Train and Pred\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize model\n",
        "model = ABCNN()\n",
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total number of trainable parameters: {total_params}\")\n",
        "\n",
        "# Define optimizer, scheduler, and loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.001)\n",
        "scheduler = StepLR(optimizer, step_size=20, gamma=0.3)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "num_epochs = 30\n",
        "\n",
        "# Variables to track predictions and labels\n",
        "all_predictions = []\n",
        "all_true_labels = []\n",
        "best_val_loss = float('inf')\n",
        "best_model_path = \"best_model.pth\"\n",
        "\n",
        "# Training and validation loop\n",
        "for epoch in range(num_epochs):\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for c_batch, target_batch in dev_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(c_batch)\n",
        "        loss = criterion(outputs, target_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = running_loss / len(dev_loader)\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # Validation phase on holdout set\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct_normal = 0\n",
        "    total_samples = 0\n",
        "    off_by_1_count = 0\n",
        "    off_by_2_count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for c_batch, target_batch in holdout_loader:\n",
        "            outputs = model(c_batch)\n",
        "            loss = criterion(outputs, target_batch)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            predicted = torch.sigmoid(outputs) > 0.5\n",
        "            for i in range(len(predicted)):\n",
        "                predicted_int = predicted[i].cpu().int().numpy().sum()\n",
        "                target_int = target_batch[i].cpu().int().numpy().sum()\n",
        "\n",
        "                all_predictions.append(predicted_int)\n",
        "                all_true_labels.append(target_int)\n",
        "\n",
        "            # Track accuracy\n",
        "            for i in range(len(target_batch)):\n",
        "                target_values = target_batch[i]\n",
        "                predicted_values = predicted[i]\n",
        "                if torch.all(target_values == predicted_values):\n",
        "                    correct_normal += 1\n",
        "\n",
        "                abs_diff = torch.abs(predicted_values.int() - target_values.int()).sum()\n",
        "                off_by_1_count += (abs_diff == 1).item()\n",
        "                off_by_2_count += (abs_diff == 2).item()\n",
        "                total_samples += 1\n",
        "\n",
        "    avg_val_loss = val_loss / len(holdout_loader)\n",
        "    normal_accuracy = correct_normal / total_samples\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Validation Loss: {avg_val_loss:.4f}\")\n",
        "    print(f\"Normal Accuracy: {normal_accuracy:.4f}\")\n",
        "    print(f\"Off-by-1 Accuracy: {(off_by_1_count / total_samples):.4f}\")\n",
        "    print(f\"Off-by-2 Accuracy: {(off_by_2_count / total_samples):.4f}\")\n",
        "\n",
        "    # Save the best model\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(f\"Saved best model with validation loss {best_val_loss:.4f}\")\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "# Save predictions and true labels for holdout set\n",
        "predictions_df = pd.DataFrame({\n",
        "    \"True Labels\": all_true_labels,\n",
        "    \"Predictions\": all_predictions\n",
        "})\n",
        "predictions_df.to_csv(\"train_test_predictions.csv\", index=False)\n",
        "print(\"Predictions saved to train_test_predictions.csv\")\n",
        "\n",
        "\n",
        "# Load the best model\n",
        "print(\"\\nLoading the best model for big test predictions...\")\n",
        "model = ABCNN()  # Replace with your model class\n",
        "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
        "model.eval()\n",
        "\n",
        "# Generate predictions for big test loader\n",
        "print(\"\\nGenerating predictions for big test set...\")\n",
        "big_test_predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for idx, (c_batch, target_batch) in enumerate(big_test_loader):\n",
        "        try:\n",
        "            # Ensure c_batch is a tensor\n",
        "            if isinstance(c_batch, list):\n",
        "                c_batch = torch.stack([torch.tensor(x, dtype=torch.float32) for x in c_batch])\n",
        "\n",
        "            # Run the model and make predictions\n",
        "            outputs = model(c_batch)\n",
        "            predicted = torch.sigmoid(outputs) > 0.5\n",
        "            # Sum the binary output along the dimension (e.g., [1,1,1,0,0,0] -> 3)\n",
        "            summed_predictions = predicted.int().sum(dim=1).cpu().numpy()\n",
        "            big_test_predictions.extend(summed_predictions.tolist())\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing batch {idx}: {e}\")\n",
        "            continue\n",
        "\n",
        "# Save big test set predictions as a single number\n",
        "big_test_df = pd.DataFrame(big_test_predictions, columns=[\"Prediction\"])\n",
        "big_test_df.to_csv(\"big_pred.csv\", index=False)\n",
        "print(\"Big test set predictions saved to big_pred.csv\")\n",
        "\n",
        "# Convert to JSON format\n",
        "json_data = pd.Series(big_test_df['Prediction'].values - 1).to_json(orient='index')\n",
        "\n",
        "# Specify the file path to save the JSON data\n",
        "file_path = 'Deep Breathe_output.json'\n",
        "\n",
        "# Write the JSON data to the file\n",
        "with open(file_path, 'w') as f:\n",
        "    f.write(json_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TaA3NgI1Du1i",
        "outputId": "6661b309-6ca3-4350-b4ac-5effd8a1fd47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of trainable parameters: 315078\n",
            "Epoch 1/30, Training Loss: 0.4178\n",
            "Epoch 1/30, Validation Loss: 0.3800\n",
            "Normal Accuracy: 0.3029\n",
            "Off-by-1 Accuracy: 0.4400\n",
            "Off-by-2 Accuracy: 0.2000\n",
            "Saved best model with validation loss 0.3800\n",
            "Epoch 2/30, Training Loss: 0.3074\n",
            "Epoch 2/30, Validation Loss: 0.2858\n",
            "Normal Accuracy: 0.3514\n",
            "Off-by-1 Accuracy: 0.4971\n",
            "Off-by-2 Accuracy: 0.1371\n",
            "Saved best model with validation loss 0.2858\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}